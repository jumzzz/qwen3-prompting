{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Separating Data and Instructions\n",
    "\n",
    "- [Lesson](#lesson)\n",
    "- [Exercises](#exercises)\n",
    "- [Example Playground](#example-playground)\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run the following setup cell to load your API key and establish the `get_completion` helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, stream_generate\n",
    "\n",
    "model_path = \"../../huggingface-models/qwen3-8b-4bit/\"\n",
    "MODEL, TOKENIZER = load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_markdown(out, header=\"Assistant\"):\n",
    "    markdown_out = \"---\" + f\"\\n##### {header}\\n{out}\\n\"\n",
    "    display(Markdown(markdown_out))\n",
    "\n",
    "def display_chat_exchange(user_prompt, assistant_response):\n",
    "    markdown_out = \"---\\n#### Exchange (User/Assistant)\\n\" + f\"### üê∂ \\n\\n{user_prompt} \\n### ü§ñ \\n{assistant_response}\\n\\n\" + \"---\" \n",
    "    display(Markdown(markdown_out))\n",
    "\n",
    "def display_chat_exchange_raw(user_prompt, assistant_response):\n",
    "    raw_msg = f\"üê∂\\n{user_prompt}\\n\\nü§ñ\\n{assistant_response}\\n\"\n",
    "    markdown_msg = f\"```text\\n{raw_msg}\\n```\"\n",
    "    display(Markdown(markdown_msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(\n",
    "        prompt: str, \n",
    "        system_prompt=\"\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    tokenized_prompt = TOKENIZER.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False\n",
    "    )\n",
    "\n",
    "    return TOKENIZER.decode(tokenized_prompt),  \"\".join([\n",
    "        response.text\n",
    "        for response in stream_generate(\n",
    "            MODEL, \n",
    "            TOKENIZER, \n",
    "            tokenized_prompt, \n",
    "            max_tokens=-1 \n",
    "        )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lesson\n",
    "\n",
    "Oftentimes, we don't want to write full prompts, but instead want **prompt templates that can be modified later with additional input data before submitting to Claude**. This might come in handy if you want Claude to do the same thing every time, but the data that Claude uses for its task might be different each time. \n",
    "\n",
    "Luckily, we can do this pretty easily by **separating the fixed skeleton of the prompt from variable user input, then substituting the user input into the prompt** before sending the full prompt to Claude. \n",
    "\n",
    "Below, we'll walk step by step through how to write a substitutable prompt template, as well as how to substitute in user input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "In this first example, we're asking Claude to act as an animal noise generator. Notice that the full prompt submitted to Claude is just the `PROMPT_TEMPLATE` substituted with the input (in this case, \"Cow\"). Notice that the word \"Cow\" replaces the `ANIMAL` placeholder via an f-string when we print out the full prompt.\n",
    "\n",
    "**Note:** You don't have to call your placeholder variable anything in particular in practice. We called it `ANIMAL` in this example, but just as easily, we could have called it `CREATURE` or `A` (although it's generally good to have your variable names be specific and relevant so that your prompt template is easy to understand even without the substitution, just for user parseability). Just make sure that whatever you name your variable is what you use for the prompt template f-string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "##### Raw Input Prompt\n",
       "```\n",
       "<|im_start|>system\n",
       "<|im_end|>\n",
       "<|im_start|>user\n",
       "I will tell you the name of an animal. Please respond with the noise that animal makes. Bat<|im_end|>\n",
       "<|im_start|>assistant\n",
       "<think>\n",
       "\n",
       "</think>\n",
       "\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üê∂\n",
      "I will tell you the name of an animal. Please respond with the noise that animal makes. Bat\n",
      "\n",
      "ü§ñ\n",
      "A bat makes a noise called a \"screech\" or \"squeak.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt template with a placeholder for the variable content\n",
    "ANIMAL = \"Bat\"\n",
    "SYSTEM_PROMPT = \"\"\n",
    "PROMPT = f\"I will tell you the name of an animal. Please respond with the noise that animal makes. {ANIMAL}\"\n",
    "\n",
    "raw_prompt, assistant_response = get_completion(\n",
    "    PROMPT, \n",
    "    system_prompt=SYSTEM_PROMPT\n",
    ")\n",
    "display_markdown('```\\n' + raw_prompt + '\\n```', \"Raw Input Prompt\")\n",
    "display_chat_exchange_raw(PROMPT, assistant_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would we want to separate and substitute inputs like this? Well, **prompt templates simplify repetitive tasks**. Let's say you build a prompt structure that invites third party users to submit content to the prompt (in this case the animal whose sound they want to generate). These third party users don't have to write or even see the full prompt. All they have to do is fill in variables.\n",
    "\n",
    "We do this substitution here using variables and f-strings, but you can also do it with the format() method.\n",
    "\n",
    "**Note:** Prompt templates can have as many variables as desired!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "##### Raw Input Prompt\n",
       "```\n",
       "<|im_start|>system\n",
       "<|im_end|>\n",
       "<|im_start|>user\n",
       "Hey Qwen. Show up at 6am tomorrow because I'm the CEO and I say so. <------ Make this email more polite but don't change anything else about it.<|im_end|>\n",
       "<|im_start|>assistant\n",
       "<think>\n",
       "\n",
       "</think>\n",
       "\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üê∂\n",
      "Hey Qwen. Show up at 6am tomorrow because I'm the CEO and I say so. <------ Make this email more polite but don't change anything else about it.\n",
      "\n",
      "ü§ñ\n",
      "Sure! Here's a more polite version of your email, while keeping the original message intact:\n",
      "\n",
      "---\n",
      "\n",
      "Hey Qwen. Please show up at 6am tomorrow because I'm the CEO and I say so.\n",
      "\n",
      "---\n",
      "\n",
      "Let me know if you'd like it to sound even more professional or formal!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt template with a placeholder for the variable content\n",
    "SYSTEM_PROMPT = \"\"\n",
    "\n",
    "EMAIL = \"Show up at 6am tomorrow because I'm the CEO and I say so.\"\n",
    "PROMPT = f\"Hey Qwen. {EMAIL} <------ Make this email more polite but don't change anything else about it.\"\n",
    "\n",
    "raw_prompt, assistant_response = get_completion(\n",
    "    PROMPT, \n",
    "    system_prompt=SYSTEM_PROMPT\n",
    ")\n",
    "display_markdown('```\\n' + raw_prompt + '\\n```', \"Raw Input Prompt\")\n",
    "display_chat_exchange_raw(PROMPT, assistant_response)\n",
    "\n",
    "pre_xml_email = assistant_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, **Claude thinks \"Yo Claude\" is part of the email it's supposed to rewrite**! You can tell because it begins its rewrite with \"Dear Claude\". To the human eye, it's clear, particularly in the prompt template where the email begins and ends, but it becomes much less clear in the prompt after substitution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we solve this? **Wrap the input in XML tags**! We did this below, and as you can see, there's no more \"Dear Claude\" in the output.\n",
    "\n",
    "[XML tags](https://docs.anthropic.com/claude/docs/use-xml-tags) are angle-bracket tags like `<tag></tag>`. They come in pairs and consist of an opening tag, such as `<tag>`, and a closing tag marked by a `/`, such as `</tag>`. XML tags are used to wrap around content, like this: `<tag>content</tag>`.\n",
    "\n",
    "**Note:** While Claude can recognize and work with a wide range of separators and delimeters, we recommend that you **use specifically XML tags as separators** for Claude, as Claude was trained specifically to recognize XML tags as a prompt organizing mechanism. Outside of function calling, **there are no special sauce XML tags that Claude has been trained on that you should use to maximally boost your performance**. We have purposefully made Claude very malleable and customizable this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "##### Raw Input Prompt\n",
       "```\n",
       "<|im_start|>system\n",
       "<|im_end|>\n",
       "<|im_start|>user\n",
       "Yo Qwen. <email>Show up at 6am tomorrow because I'm the CEO and I say so.</email> <------ Make this email more polite but don't change anything else about it.<|im_end|>\n",
       "<|im_start|>assistant\n",
       "<think>\n",
       "\n",
       "</think>\n",
       "\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üê∂\n",
      "Yo Qwen. <email>Show up at 6am tomorrow because I'm the CEO and I say so.</email> <------ Make this email more polite but don't change anything else about it.\n",
      "\n",
      "ü§ñ\n",
      "Sure! Here's a more polite version of your email, while keeping the original message intact:\n",
      "\n",
      "---\n",
      "\n",
      "**Subject:** Reminder: Show up at 6am Tomorrow\n",
      "\n",
      "Hi [Name],\n",
      "\n",
      "Please show up at 6am tomorrow because I'm the CEO and I say so.\n",
      "\n",
      "Best regards,  \n",
      "[Your Name]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prompt template with a placeholder for the variable content\n",
    "SYSTEM_PROMPT = \"\"\n",
    "\n",
    "EMAIL = \"Show up at 6am tomorrow because I'm the CEO and I say so.\"\n",
    "PROMPT = f\"Yo Qwen. <email>{EMAIL}</email> <------ Make this email more polite but don't change anything else about it.\"\n",
    "\n",
    "raw_prompt, assistant_response = get_completion(\n",
    "    PROMPT, \n",
    "    system_prompt=SYSTEM_PROMPT\n",
    ")\n",
    "display_markdown('```\\n' + raw_prompt + '\\n```', \"Raw Input Prompt\")\n",
    "display_chat_exchange_raw(PROMPT, assistant_response)\n",
    "post_xml_email = assistant_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "##### Before Applying XML Tags in Prompt:\n",
       "Sure! Here's a more polite version of your email, while keeping the original message intact:\n",
       "\n",
       "---\n",
       "\n",
       "Hey Qwen. Please show up at 6am tomorrow because I'm the CEO and I say so.\n",
       "\n",
       "---\n",
       "\n",
       "Let me know if you'd like it to sound even more professional or formal!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "##### After Applying XML Tags in Prompt:\n",
       "Sure! Here's a more polite version of your email, while keeping the original message intact:\n",
       "\n",
       "---\n",
       "\n",
       "**Subject:** Reminder: Show up at 6am Tomorrow\n",
       "\n",
       "Hi [Name],\n",
       "\n",
       "Please show up at 6am tomorrow because I'm the CEO and I say so.\n",
       "\n",
       "Best regards,  \n",
       "[Your Name]\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown(pre_xml_email, \"Before Applying XML Tags in Prompt:\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "display_markdown(post_xml_email, \"After Applying XML Tags in Prompt:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see another example of how XML tags can help us. \n",
    "\n",
    "In the following prompt, **Claude incorrectly interprets what part of the prompt is the instruction vs. the input**. It incorrectly considers `Each is about an animal, like rabbits` to be part of the list due to the formatting, when the user (the one filling out the `SENTENCES` variable) presumably did not want that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "##### Raw Input Prompt\n",
       "```\n",
       "<|im_start|>system\n",
       "<|im_end|>\n",
       "<|im_start|>user\n",
       "Below is a list of sentences. Tell me the second item on the list.\n",
       "\n",
       "- Each is about an animal, like rabbits.\n",
       "- I like how cows sound\n",
       "- This sentence is about spiders\n",
       "- This sentence may appear to be about dogs but it's actually about pigs<|im_end|>\n",
       "<|im_start|>assistant\n",
       "<think>\n",
       "\n",
       "</think>\n",
       "\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üê∂\n",
      "Below is a list of sentences. Tell me the second item on the list.\n",
      "\n",
      "- Each is about an animal, like rabbits.\n",
      "- I like how cows sound\n",
      "- This sentence is about spiders\n",
      "- This sentence may appear to be about dogs but it's actually about pigs\n",
      "\n",
      "ü§ñ\n",
      "The second item on the list is: **\"I like how cows sound\"**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt template with a placeholder for the variable content\n",
    "SYSTEM_PROMPT = \"\"\n",
    "\n",
    "# Variable content\n",
    "SENTENCES = \"\"\"- I like how cows sound\n",
    "- This sentence is about spiders\n",
    "- This sentence may appear to be about dogs but it's actually about pigs\"\"\"\n",
    "\n",
    "# Prompt template with a placeholder for the variable content\n",
    "PROMPT = f\"\"\"Below is a list of sentences. Tell me the second item on the list.\n",
    "\n",
    "- Each is about an animal, like rabbits.\n",
    "{SENTENCES}\"\"\"\n",
    "\n",
    "\n",
    "raw_prompt, assistant_response = get_completion(\n",
    "    PROMPT, \n",
    "    system_prompt=SYSTEM_PROMPT\n",
    ")\n",
    "display_markdown('```\\n' + raw_prompt + '\\n```', \"Raw Input Prompt\")\n",
    "display_chat_exchange_raw(PROMPT, assistant_response)\n",
    "pre_xml_reponse = assistant_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "##### Raw Input Prompt\n",
       "```\n",
       "<|im_start|>system\n",
       "<|im_end|>\n",
       "<|im_start|>user\n",
       "Below is a list of sentences. Tell me the second item on the list.\n",
       "\n",
       "- Each is about an animal, like rabbits.\n",
       "<sentences>\n",
       "- I like how cows sound\n",
       "- This sentence is about spiders\n",
       "- This sentence may appear to be about dogs but it's actually about pigs\n",
       "</sentences><|im_end|>\n",
       "<|im_start|>assistant\n",
       "<think>\n",
       "\n",
       "</think>\n",
       "\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üê∂\n",
      "Below is a list of sentences. Tell me the second item on the list.\n",
      "\n",
      "- Each is about an animal, like rabbits.\n",
      "<sentences>\n",
      "- I like how cows sound\n",
      "- This sentence is about spiders\n",
      "- This sentence may appear to be about dogs but it's actually about pigs\n",
      "</sentences>\n",
      "\n",
      "ü§ñ\n",
      "The second item on the list is:  \n",
      "**\"This sentence is about spiders\"**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt template with a placeholder for the variable content\n",
    "SYSTEM_PROMPT = \"\"\n",
    "\n",
    "# Variable content\n",
    "SENTENCES = \"\"\"- I like how cows sound\n",
    "- This sentence is about spiders\n",
    "- This sentence may appear to be about dogs but it's actually about pigs\"\"\"\n",
    "\n",
    "# Prompt template with a placeholder for the variable content\n",
    "PROMPT = f\"\"\"Below is a list of sentences. Tell me the second item on the list.\n",
    "\n",
    "- Each is about an animal, like rabbits.\n",
    "<sentences>\n",
    "{SENTENCES}\n",
    "</sentences>\"\"\"\n",
    "\n",
    "\n",
    "raw_prompt, assistant_response = get_completion(\n",
    "    PROMPT, \n",
    "    system_prompt=SYSTEM_PROMPT\n",
    ")\n",
    "display_markdown('```\\n' + raw_prompt + '\\n```', \"Raw Input Prompt\")\n",
    "display_chat_exchange_raw(PROMPT, assistant_response)\n",
    "pre_xml_reponse = assistant_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In the incorrect version of the \"Each is about an animal\" prompt, we had to include the hyphen to get Claude to respond incorrectly in the way we wanted to for this example. This is an important lesson about prompting: **small details matter**! It's always worth it to **scrub your prompts for typos and grammatical errors**. Claude is sensitive to patterns (in its early years, before finetuning, it was a raw text-prediction tool), and it's more likely to make mistakes when you make mistakes, smarter when you sound smart, sillier when you sound silly, and so on.\n",
    "\n",
    "If you would like to experiment with the lesson prompts without changing any content above, scroll all the way to the bottom of the lesson notebook to visit the [**Example Playground**](#example-playground)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "- [Exercise 4.1 - Haiku Topic](#exercise-41---haiku-topic)\n",
    "- [Exercise 4.2 - Dog Question with Typos](#exercise-42---dog-question-with-typos)\n",
    "- [Exercise 4.3 - Dog Question Part 2](#exercise-42---dog-question-part-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1 - Haiku Topic\n",
    "Modify the `PROMPT` so that it's a template that will take in a variable called `TOPIC` and output a haiku about the topic. This exercise is just meant to test your understanding of the variable templating structure with f-strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "##### Raw Input Prompt\n",
       "```\n",
       "<|im_start|>system\n",
       "<|im_end|>\n",
       "<|im_start|>user\n",
       "Make a Haiku for Pigs<|im_end|>\n",
       "<|im_start|>assistant\n",
       "<think>\n",
       "\n",
       "</think>\n",
       "\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üê∂\n",
      "Make a Haiku for Pigs\n",
      "\n",
      "ü§ñ\n",
      "Soft snouts in the mud,  \n",
      "A squeal echoes through the barn‚Äî  \n",
      "Dreams of bacon and sun.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Variable content\n",
    "TOPIC = \"Pigs\"\n",
    "\n",
    "# Prompt template with a placeholder for the variable content\n",
    "PROMPT = f\"Make a Haiku for <topic>{TOPIC}</topic>\"\n",
    "\n",
    "# Get Claude's response\n",
    "\n",
    "raw_prompt, assistant_response = get_completion(\n",
    "    PROMPT, \n",
    "    system_prompt=SYSTEM_PROMPT\n",
    ")\n",
    "display_markdown('```\\n' + raw_prompt + '\\n```', \"Raw Input Prompt\")\n",
    "display_chat_exchange_raw(PROMPT, assistant_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------- Full prompt with variable substutions ---------------------------\n",
      "Make a Haiku for Pigs\n",
      "\n",
      "------------------------------------- Claude's response -------------------------------------\n",
      "Soft snouts in the mud,  \n",
      "A squeal echoes through the barn‚Äî  \n",
      "Dreams of bacon and sun.\n",
      "\n",
      "------------------------------------------ GRADING ------------------------------------------\n",
      "This exercise has been correctly solved: True\n"
     ]
    }
   ],
   "source": [
    "# Function to grade exercise correctness\n",
    "def grade_exercise(text):\n",
    "    v1 = bool(re.search(\"pigs\", text.lower())) \n",
    "    v2 = bool(re.search(\"haiku\", text.lower()))\n",
    "    v3 = bool(re.search(\"mud\", text.lower()))\n",
    "    return v1 or v2 or v3\n",
    "\n",
    "# Print Claude's response\n",
    "print(\"--------------------------- Full prompt with variable substutions ---------------------------\")\n",
    "print(PROMPT)\n",
    "print(\"\\n------------------------------------- Claude's response -------------------------------------\")\n",
    "print(assistant_response)\n",
    "print(\"\\n------------------------------------------ GRADING ------------------------------------------\")\n",
    "print(\"This exercise has been correctly solved:\", grade_exercise(assistant_response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2 - Dog Question with Typos\n",
    "Fix the `PROMPT` by adding XML tags so that Claude produces the right answer. \n",
    "\n",
    "Try not to change anything else about the prompt. The messy and mistake-ridden writing is intentional, so you can see how Claude reacts to such mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "##### Raw Input Prompt\n",
       "```\n",
       "<|im_start|>system\n",
       "<|im_end|>\n",
       "<|im_start|>user\n",
       "Hia its me i have a q about dogs jkaerjv <question>ar cn brown?</question> jklmvca tx it help me muhch much atx fst fst answer short short tx<|im_end|>\n",
       "<|im_start|>assistant\n",
       "<think>\n",
       "\n",
       "</think>\n",
       "\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üê∂\n",
      "Hia its me i have a q about dogs jkaerjv <question>ar cn brown?</question> jklmvca tx it help me muhch much atx fst fst answer short short tx\n",
      "\n",
      "ü§ñ\n",
      "Sure! Here's a short answer to your question:\n",
      "\n",
      "**\"Can brown dogs be?\"**  \n",
      "It seems like your question is incomplete. Could you clarify what you're asking about brown dogs?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Variable content\n",
    "QUESTION = \"ar cn brown?\"\n",
    "\n",
    "# Prompt template with a placeholder for the variable content\n",
    "PROMPT = f\"Hia its me i have a q about dogs jkaerjv <question>{QUESTION}</question> jklmvca tx it help me muhch much atx fst fst answer short short tx\"\n",
    "\n",
    "raw_prompt, assistant_response = get_completion(\n",
    "    PROMPT, \n",
    "    system_prompt=SYSTEM_PROMPT\n",
    ")\n",
    "display_markdown('```\\n' + raw_prompt + '\\n```', \"Raw Input Prompt\")\n",
    "display_chat_exchange_raw(PROMPT, assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------- Full prompt with variable substutions ---------------------------\n",
      "Hia its me i have a q about dogs jkaerjv <question>ar cn brown?</question> jklmvca tx it help me muhch much atx fst fst answer short short tx\n",
      "\n",
      "------------------------------------- Claude's response -------------------------------------\n",
      "Sure! Here's a short answer to your question:\n",
      "\n",
      "**\"Can brown dogs be?\"**  \n",
      "It seems like your question is incomplete. Could you clarify what you're asking about brown dogs?\n",
      "\n",
      "------------------------------------------ GRADING ------------------------------------------\n",
      "This exercise has been correctly solved: True\n"
     ]
    }
   ],
   "source": [
    "# Function to grade exercise correctness\n",
    "def grade_exercise(text):\n",
    "    return bool(re.search(\"brown\", text.lower()))\n",
    "\n",
    "# Print Claude's response\n",
    "print(\"--------------------------- Full prompt with variable substutions ---------------------------\")\n",
    "print(PROMPT)\n",
    "print(\"\\n------------------------------------- Claude's response -------------------------------------\")\n",
    "print(assistant_response)\n",
    "print(\"\\n------------------------------------------ GRADING ------------------------------------------\")\n",
    "print(\"This exercise has been correctly solved:\", grade_exercise(assistant_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.3 - Dog Question Part 2\n",
    "Fix the `PROMPT` **WITHOUT** adding XML tags. Instead, remove only one or two words from the prompt.\n",
    "\n",
    "Just as with the above exercises, try not to change anything else about the prompt. This will show you what kind of language Claude can parse and understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "##### Raw Input Prompt\n",
       "```\n",
       "<|im_start|>system\n",
       "<|im_end|>\n",
       "<|im_start|>user\n",
       "Hia its me i have a q about dogs ar cn brown? tx it help me muhch much atx fst fst answer short short tx<|im_end|>\n",
       "<|im_start|>assistant\n",
       "<think>\n",
       "\n",
       "</think>\n",
       "\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üê∂\n",
      "Hia its me i have a q about dogs ar cn brown? tx it help me muhch much atx fst fst answer short short tx\n",
      "\n",
      "ü§ñ\n",
      "Can dogs be brown? Yes, dogs can have brown fur.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Variable content\n",
    "QUESTION = \"ar cn brown?\"\n",
    "\n",
    "# Prompt template with a placeholder for the variable content\n",
    "PROMPT = f\"Hia its me i have a q about dogs {QUESTION} tx it help me muhch much atx fst fst answer short short tx\"\n",
    "\n",
    "raw_prompt, assistant_response = get_completion(\n",
    "    PROMPT, \n",
    "    system_prompt=SYSTEM_PROMPT\n",
    ")\n",
    "display_markdown('```\\n' + raw_prompt + '\\n```', \"Raw Input Prompt\")\n",
    "display_chat_exchange_raw(PROMPT, assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------- Full prompt with variable substutions ---------------------------\n",
      "Hia its me i have a q about dogs ar cn brown? tx it help me muhch much atx fst fst answer short short tx\n",
      "\n",
      "------------------------------------- Claude's response -------------------------------------\n",
      "Can dogs be brown? Yes, dogs can have brown fur.\n",
      "\n",
      "------------------------------------------ GRADING ------------------------------------------\n",
      "This exercise has been correctly solved: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to grade exercise correctness\n",
    "def grade_exercise(text):\n",
    "    return bool(re.search(\"brown\", text.lower()))\n",
    "\n",
    "# Print Claude's response\n",
    "print(\"--------------------------- Full prompt with variable substutions ---------------------------\")\n",
    "print(PROMPT)\n",
    "print(\"\\n------------------------------------- Claude's response -------------------------------------\")\n",
    "print(assistant_response)\n",
    "print(\"\\n------------------------------------------ GRADING ------------------------------------------\")\n",
    "print(\"This exercise has been correctly solved:\", grade_exercise(assistant_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
