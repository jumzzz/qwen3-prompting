{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Formatting Output and Speaking for Claude\n",
    "\n",
    "- [Lesson](#lesson)\n",
    "- [Exercises](#exercises)\n",
    "- [Example Playground](#example-playground)\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run the following setup cell to load your API key and establish the `get_completion` helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from mlx_lm import load, stream_generate\n",
    "\n",
    "model_path = \"../../huggingface-models/qwen3-8b-4bit/\"\n",
    "MODEL, TOKENIZER = load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_markdown(out, header=\"Assistant\"):\n",
    "    markdown_out = \"---\" + f\"\\n##### {header}\\n{out}\\n\"\n",
    "    display(Markdown(markdown_out))\n",
    "\n",
    "def display_chat_exchange(user_prompt, assistant_response):\n",
    "    markdown_out = \"---\\n#### Exchange (User/Assistant)\\n\" + f\"### üê∂ \\n\\n{user_prompt} \\n### ü§ñ \\n{assistant_response}\\n\\n\" + \"---\" \n",
    "    display(Markdown(markdown_out))\n",
    "\n",
    "def display_chat_exchange_raw(user_prompt, assistant_response):\n",
    "    raw_msg = f\"üê∂\\n{user_prompt}\\n\\nü§ñ\\n{assistant_response}\\n\"\n",
    "    markdown_msg = f\"```text\\n{raw_msg}\\n```\"\n",
    "    display(Markdown(markdown_msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(\n",
    "        prompt: str, \n",
    "        system_prompt=\"\", prefill=\"\", \n",
    "        max_tokens=32768, \n",
    "        enable_thinking=False,\n",
    "        print_stream=False):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": prefill},\n",
    "    ]\n",
    "    tokenized_prompt = TOKENIZER.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=enable_thinking\n",
    "    )\n",
    "\n",
    "    total_response = []\n",
    "\n",
    "    for response in stream_generate(\n",
    "        MODEL, \n",
    "        TOKENIZER, \n",
    "        tokenized_prompt, \n",
    "        max_tokens=max_tokens \n",
    "    ):\n",
    "        total_response.append(response.text)\n",
    "        if print_stream:\n",
    "            print(response.text, end=\"\")\n",
    "\n",
    "    return TOKENIZER.decode(tokenized_prompt), \"\".join(total_response) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lesson\n",
    "\n",
    "**Claude can format its output in a wide variety of ways**. You just need to ask for it to do so!\n",
    "\n",
    "One of these ways is by using XML tags to separate out the response from any other superfluous text. You've already learned that you can use XML tags to make your prompt clearer and more parseable to Claude. It turns out, you can also ask Claude to **use XML tags to make its output clearer and more easily understandable** to humans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Examples\n",
    "\n",
    "Remember the 'poem preamble problem' we solved in Chapter 2 by asking Claude to skip the preamble entirely? It turns out we can also achieve a similar outcome by **telling Claude to put the poem in XML tags**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "##### Raw Input Prompt\n",
       "```\n",
       "<|im_start|>system\n",
       "<|im_end|>\n",
       "<|im_start|>user\n",
       "Please write a haiku about Bat. Put it in <haiku> tags.<|im_end|>\n",
       "<|im_start|>assistant\n",
       "<think>\n",
       "\n",
       "</think>\n",
       "\n",
       "<|im_end|>\n",
       "<|im_start|>assistant\n",
       "<think>\n",
       "\n",
       "</think>\n",
       "\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```text\n",
       "üê∂\n",
       "Please write a haiku about Bat. Put it in <haiku> tags.\n",
       "\n",
       "ü§ñ\n",
       "<haiku>\n",
       "Silent in the dark,  \n",
       "eyes glow like distant stars‚Äî  \n",
       "shadow's gentle friend.\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prompt template with a placeholder for the variable content\n",
    "ANIMAL = \"Bat\"\n",
    "SYSTEM_PROMPT = \"\"\n",
    "PROMPT = f\"Please write a haiku about {ANIMAL}. Put it in <haiku> tags.\"\n",
    "\n",
    "raw_prompt, assistant_response = get_completion(\n",
    "    PROMPT, \n",
    "    system_prompt=SYSTEM_PROMPT\n",
    ")\n",
    "display_markdown('```\\n' + raw_prompt + '\\n```', \"Raw Input Prompt\")\n",
    "display_chat_exchange_raw(PROMPT, assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
