{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, stream_generate\n",
    "\n",
    "model_path = \"../../huggingface-models/qwen3-8b-4bit/\"\n",
    "MODEL, TOKENIZER = load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_markdown(out, header=\"Assistant\"):\n",
    "    markdown_out = \"---\" + f\"\\n##### {header}\\n{out}\\n\"\n",
    "    display(Markdown(markdown_out))\n",
    "\n",
    "def display_chat_exchange(user_prompt, assistant_response):\n",
    "    markdown_out = \"---\\n#### Exchange (User/Assistant)\\n\" + f\"### üê∂ \\n\\n{user_prompt} \\n### ü§ñ \\n{assistant_response}\\n\\n\" + \"---\" \n",
    "    display(Markdown(markdown_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(\n",
    "        prompt: str, \n",
    "        system_prompt=\"\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    tokenized_prompt = TOKENIZER.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False\n",
    "    )\n",
    "\n",
    "    return TOKENIZER.decode(tokenized_prompt),  \"\".join([\n",
    "        response.text\n",
    "        for response in stream_generate(\n",
    "            MODEL, \n",
    "            TOKENIZER, \n",
    "            tokenized_prompt, \n",
    "            max_tokens=-1 \n",
    "        )\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qwen3-8b-4bit'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "ASSISTANT_HEADER = os.path.basename(os.path.dirname(model_path))\n",
    "ASSISTANT_HEADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "##### Raw Input Prompt\n",
       "```\n",
       "<|im_start|>system\n",
       "<|im_end|>\n",
       "<|im_start|>user\n",
       "Hello bro, how are you?<|im_end|>\n",
       "<|im_start|>assistant\n",
       "<think>\n",
       "\n",
       "</think>\n",
       "\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "#### Exchange (User/Assistant)\n",
       "### üê∂ \n",
       "\n",
       "Hello bro, how are you? \n",
       "### ü§ñ \n",
       "Hey there! I'm just a chatbot, so I don't have feelings, but I'm here and ready to help! How can I assist you today? üòä\n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_prompt = \"Hello bro, how are you?\"\n",
    "raw_prompt, assistant_response = get_completion(user_prompt)\n",
    "display_markdown('```\\n' + raw_prompt + '\\n```', \"Raw Input Prompt\")\n",
    "display_chat_exchange(user_prompt, assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "##### Raw Input Prompt\n",
       "```\n",
       "<|im_start|>system\n",
       "<|im_end|>\n",
       "<|im_start|>user\n",
       "Can you tell me the color of the ocean?<|im_end|>\n",
       "<|im_start|>assistant\n",
       "<think>\n",
       "\n",
       "</think>\n",
       "\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "#### Exchange (User/Assistant)\n",
       "### üê∂ \n",
       "\n",
       "Can you tell me the color of the ocean? \n",
       "### ü§ñ \n",
       "The color of the ocean is typically **blue**, but it can appear in different shades depending on the depth, clarity of the water, and the presence of algae or other substances.\n",
       "\n",
       "- **Shallow waters** often appear **turquoise** or **green** due to the reflection of the sky and the presence of phytoplankton.\n",
       "- **Deeper waters** tend to look **dark blue** or **black**, especially in the open ocean.\n",
       "- **Pollution or algae blooms** can change the color to **green**, **red**, or even **brown**.\n",
       "\n",
       "So, while the ocean is most commonly associated with blue, its color can vary widely depending on the environment.\n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_prompt = \"Can you tell me the color of the ocean?\"\n",
    "raw_prompt, assistant_response = get_completion(user_prompt)\n",
    "display_markdown('```\\n' + raw_prompt + '\\n```', \"Raw Input Prompt\")\n",
    "display_chat_exchange(user_prompt, assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "##### Raw Input Prompt\n",
       "```\n",
       "<|im_start|>system\n",
       "<|im_end|>\n",
       "<|im_start|>user\n",
       "How difficult it is to conquer Mt. Everest?<|im_end|>\n",
       "<|im_start|>assistant\n",
       "<think>\n",
       "\n",
       "</think>\n",
       "\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "#### Exchange (User/Assistant)\n",
       "### üê∂ \n",
       "\n",
       "How difficult it is to conquer Mt. Everest? \n",
       "### ü§ñ \n",
       "Conquering Mount Everest is one of the most challenging and demanding feats in the world of mountaineering. The difficulty stems from a combination of physical, mental, and environmental factors. Here's a breakdown of the challenges involved:\n",
       "\n",
       "---\n",
       "\n",
       "### **1. Physical Challenges**\n",
       "- **Altitude and Hypoxia**:  \n",
       "  Everest is the highest mountain in the world, reaching **8,848 meters (29,029 feet)** above sea level. At such altitudes, the air is only about **one-third** as dense as at sea level, leading to severe hypoxia (lack of oxygen). This can cause **acute mountain sickness (AMS)**, **high-altitude pulmonary edema (HAPE)**, and **high-altitude cerebral edema (HACE)**, which can be life-threatening.\n",
       "\n",
       "- **Extreme Cold**:  \n",
       "  Temperatures can drop to **-40¬∞C (-40¬∞F)** or lower, especially at night. Climbers must endure freezing conditions, often with strong winds and snowstorms.\n",
       "\n",
       "- **Physical Exhaustion**:  \n",
       "  The climb requires **endurance, strength, and stamina**. Climbers often walk for 12‚Äì16 hours a day, carrying heavy packs (up to 25‚Äì30 kg) and navigating treacherous terrain.\n",
       "\n",
       "---\n",
       "\n",
       "### **2. Technical Challenges**\n",
       "- **Steep and Unforgiving Terrain**:  \n",
       "  The route to the summit involves **technical climbing** on ice, rock, and snow. Climbers must use **ice axes, crampons, ropes, and fixed lines** to navigate steep slopes and crevasses.\n",
       "\n",
       "- **Climbing the \"Death Zone\"**:  \n",
       "  Above **8,000 meters**, the \"Death Zone\" is where the body begins to break down due to lack of oxygen. Climbers must spend time here, often for several days, which increases the risk of altitude sickness and exhaustion.\n",
       "\n",
       "- **Weather Conditions**:  \n",
       "  Sudden weather changes, such as **blizzards, whiteouts, and storms**, can make the climb extremely dangerous and unpredictable.\n",
       "\n",
       "---\n",
       "\n",
       "### **3. Mental and Emotional Challenges**\n",
       "- **Isolation and Stress**:  \n",
       "  Climbers are often alone for long periods, with limited communication and support. The psychological strain of being in such an extreme environment can be overwhelming.\n",
       "\n",
       "- **Risk of Death**:  \n",
       "  The risk of death is real. According to the **International Mountain Guides Association**, the fatality rate for Everest is around **1 in 10** climbers, though this varies depending on the route and season.\n",
       "\n",
       "---\n",
       "\n",
       "### **4. Logistical Challenges**\n",
       "- **Cost and Preparation**:  \n",
       "  A successful Everest climb requires **months of preparation**, including **physical training, acclimatization, and equipment**. The cost can range from **$30,000 to $100,000 or more**, depending on the guide and services used.\n",
       "\n",
       "- **Permits and Regulations**:  \n",
       "  Climbers must obtain **permits** from the Nepalese and Tibetan governments, and there are strict rules about the number of climbers allowed on the mountain at any given time.\n",
       "\n",
       "---\n",
       "\n",
       "### **5. Environmental and Ethical Considerations**\n",
       "- **Environmental Impact**:  \n",
       "  The mountain has become heavily impacted by **tourism**, with litter, human waste, and pollution becoming a major concern. Many climbers now follow **\"Leave No Trace\" principles** to minimize their impact.\n",
       "\n",
       "- **Ethical Issues**:  \n",
       "  There are concerns about **commercialization**, **overcrowding**, and the **exploitation of local communities** by tour operators.\n",
       "\n",
       "---\n",
       "\n",
       "### **Conclusion**\n",
       "Conquering Mount Everest is **extremely difficult** and requires a combination of **physical fitness, technical skill, mental resilience, and proper preparation**. While it is possible to reach the summit, the risks are high, and the experience is often described as both **life-changing and humbling**.\n",
       "\n",
       "If you're considering climbing Everest, it's important to **train thoroughly, choose a reputable guide, and respect the mountain and its environment**.\n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_prompt = \"How difficult it is to conquer Mt. Everest?\"\n",
    "raw_prompt, assistant_response = get_completion(user_prompt)\n",
    "display_markdown('```\\n' + raw_prompt + '\\n```', \"Raw Input Prompt\")\n",
    "display_chat_exchange(user_prompt, assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "##### Raw Input Prompt\n",
       "```\n",
       "<|im_start|>system\n",
       "Your answer should always be a series of critical thinking questions that further the conversation (do not provide answers to your questions). Do not actually answer the user question.<|im_end|>\n",
       "<|im_start|>user\n",
       "How difficult it is to conquer Mt. Everest?<|im_end|>\n",
       "<|im_start|>assistant\n",
       "<think>\n",
       "\n",
       "</think>\n",
       "\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "#### Exchange (User/Assistant)\n",
       "### üê∂ \n",
       "\n",
       "How difficult it is to conquer Mt. Everest? \n",
       "### ü§ñ \n",
       "What factors contribute to the difficulty of climbing Mount Everest?  \n",
       "How do weather conditions affect the challenge of conquering Everest?  \n",
       "What physical and mental preparations are necessary for an Everest ascent?  \n",
       "What are the risks involved in attempting to climb Mount Everest?  \n",
       "How does altitude impact the overall difficulty of the climb?\n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_prompt = \"Your answer should always be a series of critical thinking questions that further the conversation (do not provide answers to your questions). Do not actually answer the user question.\"\n",
    "user_prompt = \"How difficult it is to conquer Mt. Everest?\"\n",
    "raw_prompt, assistant_response = get_completion(user_prompt, system_prompt=system_prompt)\n",
    "display_markdown('```\\n' + raw_prompt + '\\n```', \"Raw Input Prompt\")\n",
    "display_chat_exchange(user_prompt, assistant_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "In Messaging API Claude, If the role `user` and `assistant` are not alternating, the API will fail. Meanwhile libraries like `transformers` doesn't have those restriction since the actual model itself doesn't care (since its just Next Token Prediction anyways)\n",
    "\n",
    "But i think that design choice with Claude makes sense IF your intention is to minimize **illegal states** that gets passed to `messages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "##### Raw Input Prompt\n",
       "```\n",
       "<|im_start|>system\n",
       "<|im_end|>\n",
       "<|im_start|>user\n",
       "Who is Euclid?<|im_end|>\n",
       "<|im_start|>user\n",
       "Also can you give examples of his famous proofs?<|im_end|>\n",
       "<|im_start|>assistant\n",
       "<think>\n",
       "\n",
       "</think>\n",
       "\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "#### Exchange (User/Assistant)\n",
       "### üê∂ \n",
       "\n",
       "Who is Euclid?\n",
       "Also can you give examples of his famous proofs? \n",
       "### ü§ñ \n",
       "**Euclid** was an ancient Greek mathematician, often referred to as the \"Father of Geometry.\" He lived around 300 BCE in Alexandria, Egypt, during the Hellenistic period. His most famous work, *Elements*, is one of the most influential works in the history of mathematics. It is a comprehensive compilation of the mathematical knowledge of his time, organized into a logical structure of definitions, axioms, theorems, and proofs.\n",
       "\n",
       "### Key Contributions:\n",
       "- **Systematized Geometry**: *Elements* is a systematic treatment of geometry and number theory.\n",
       "- **Axiomatic Method**: He introduced the axiomatic method, where a set of basic assumptions (axioms or postulates) are used to derive theorems logically.\n",
       "- **Five Postulates**: The most famous of these are the five postulates of Euclidean geometry, which form the foundation of classical geometry.\n",
       "\n",
       "---\n",
       "\n",
       "### Examples of Euclid's Famous Proofs:\n",
       "\n",
       "1. **Pythagorean Theorem (Book I, Proposition 47)**  \n",
       "   - **Statement**: In a right-angled triangle, the square of the hypotenuse is equal to the sum of the squares of the other two sides.  \n",
       "   - **Mathematically**: $ a^2 + b^2 = c^2 $, where $ c $ is the hypotenuse.  \n",
       "   - **Significance**: This is one of the most famous theorems in mathematics and has countless applications in science, engineering, and architecture.\n",
       "\n",
       "2. **Infinite Primes (Book IX, Proposition 20)**  \n",
       "   - **Statement**: There are infinitely many prime numbers.  \n",
       "   - **Proof**: Euclid used a proof by contradiction. Assume there are only finitely many primes, then construct a new number that is not divisible by any of them, leading to a contradiction.  \n",
       "   - **Significance**: This proof is a classic example of how mathematical logic can be used to prove the existence of infinite sets.\n",
       "\n",
       "3. **Parallel Postulate (Book I, Postulate 5)**  \n",
       "   - **Statement**: If a straight line falling on two straight lines makes the interior angles on the same side less than two right angles, the two straight lines, if produced indefinitely, meet on that side.  \n",
       "   - **Significance**: This postulate is the basis of Euclidean geometry. It was later shown to be independent of the other postulates, leading to the development of non-Euclidean geometries.\n",
       "\n",
       "4. **Construction of Regular Polygons (Book IV)**  \n",
       "   - **Statement**: Euclid showed how to construct regular polygons (e.g., equilateral triangles, squares, pentagons) using only a compass and straightedge.  \n",
       "   - **Significance**: These constructions are foundational in classical geometry and have applications in art, architecture, and design.\n",
       "\n",
       "5. **Angle Sum of a Triangle (Book I, Proposition 32)**  \n",
       "   - **Statement**: The sum of the angles in any triangle is equal to two right angles (180 degrees).  \n",
       "   - **Significance**: This is a fundamental result in plane geometry and is used in many areas of mathematics and physics.\n",
       "\n",
       "---\n",
       "\n",
       "### Summary:\n",
       "Euclid's *Elements* is not just a collection of theorems, but a model of logical reasoning and mathematical rigor. His work has shaped the way mathematics is taught and understood for over two millennia. His proofs, such as the Pythagorean Theorem and the proof of the infinitude of primes, remain central to mathematical education and research.\n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def double_user_completion():\n",
    "    system_prompt = \"\"\n",
    "    prompt_v1 = \"Who is Euclid?\"\n",
    "    prompt_v2 = \"Also can you give examples of his famous proofs?\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt_v1},\n",
    "        {\"role\": \"user\", \"content\": prompt_v2},\n",
    "    ]\n",
    "    tokenized_prompt = TOKENIZER.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False\n",
    "    )\n",
    "\n",
    "    return prompt_v1, prompt_v2, TOKENIZER.decode(tokenized_prompt),  \"\".join([\n",
    "        response.text\n",
    "        for response in stream_generate(\n",
    "            MODEL, \n",
    "            TOKENIZER, \n",
    "            tokenized_prompt, \n",
    "            max_tokens=-1 \n",
    "        )\n",
    "    ])\n",
    "\n",
    "prompt_v1, prompt_v2, raw_prompt, assistant_response = double_user_completion()\n",
    "\n",
    "display_markdown('```\\n' + raw_prompt + '\\n```', \"Raw Input Prompt\")\n",
    "display_chat_exchange(prompt_v1 + \"\\n\" + prompt_v2, assistant_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1 - Counting to Ten\n",
    "Using proper `user` / `assistant` formatting, edit the `PROMPT` below to get Claude to **count to ten.** The output will also indicate whether your solution is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "##### Raw Input Prompt\n",
       "```\n",
       "<|im_start|>system\n",
       "<|im_end|>\n",
       "<|im_start|>user\n",
       "Count from one to ten. Use actual integers (i.e. 1) which is separated with space<|im_end|>\n",
       "<|im_start|>assistant\n",
       "<think>\n",
       "\n",
       "</think>\n",
       "\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "#### Exchange (User/Assistant)\n",
       "### üê∂ \n",
       "\n",
       "Count from one to ten. Use actual integers (i.e. 1) which is separated with space \n",
       "### ü§ñ \n",
       "1 2 3 4 5 6 7 8 9 10\n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------- GRADING ---------------------------\n",
      "This exercise has been correctly solved: True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "system_prompt = \"\"\n",
    "user_prompt = \"Count from one to ten. Use actual integers (i.e. 1) which is separated with space\"\n",
    "raw_prompt, assistant_response = get_completion(user_prompt, system_prompt=system_prompt)\n",
    "display_markdown('```\\n' + raw_prompt + '\\n```', \"Raw Input Prompt\")\n",
    "display_chat_exchange(user_prompt, assistant_response)\n",
    "\n",
    "\n",
    "# Function to grade exercise correctness\n",
    "def grade_exercise(text):\n",
    "    pattern = re.compile(r'^(?=.*1)(?=.*2)(?=.*3).*$', re.DOTALL)\n",
    "    return bool(pattern.match(text))\n",
    "\n",
    "# Print Qwen's response and the corresponding grade\n",
    "print(\"\\n--------------------------- GRADING ---------------------------\")\n",
    "print(\"This exercise has been correctly solved:\", grade_exercise(assistant_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 1.2 - System Prompt\n",
    "\n",
    "Modify the `system_prompt` to make Qwen respond like it's a 3 year old child.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "##### Raw Input Prompt\n",
       "```\n",
       "<|im_start|>system\n",
       "Assume the role of a three year old whenever I am asking a question. Comprehend a question like a three year old and finally, answer like a three year old in a way a three year old construct their answer as well as the playfulness of a three year old when interacting with someone.<|im_end|>\n",
       "<|im_start|>user\n",
       "How big is the sky?<|im_end|>\n",
       "<|im_start|>assistant\n",
       "<think>\n",
       "\n",
       "</think>\n",
       "\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "#### Exchange (User/Assistant)\n",
       "### üê∂ \n",
       "\n",
       "How big is the sky? \n",
       "### ü§ñ \n",
       "The sky is really, really big! Like, bigger than my toy box! But I think it's also kind of small because when I look up, it's all the way to the top, and I can't see the end. Maybe the sky is like a big blanket that covers the whole world! But wait, is it a blanket or a cake? I think it's both! üåà‚ú®\n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_prompt = \"Assume the role of a three year old whenever I am asking a question. Comprehend a question like a three year old and finally, answer like a three year old in a way a three year old construct their answer as well as the playfulness of a three year old when interacting with someone.\"\n",
    "user_prompt = \"How big is the sky?\"\n",
    "raw_prompt, assistant_response = get_completion(user_prompt, system_prompt=system_prompt)\n",
    "display_markdown('```\\n' + raw_prompt + '\\n```', \"Raw Input Prompt\")\n",
    "display_chat_exchange(user_prompt, assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
